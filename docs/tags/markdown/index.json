[
    {
        "ref": "https://bing-bai.github.io/blog/posts/differentiable-architecture-search/",
        "title": "Differentiable Architecture Search",
        "section": "posts",
        "tags": ["markdown","machine learning"],
        "date" : "2021.07.11",
        "body": "code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}  MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i = 0; i  NAS: General Problem Setup Overview   source: Elksen et al., Neural Architecture Search: A survey, 2018\n  Search Space: Defines which architecture can be represented in principle,\n  Search Strategy: Detail on how to explore search space\n often exponentially large or even unbounded    Performance Estimation Strategy: estimating an architecture\u0026rsquo;s performance\n standard training and validation of architecture on data may be computionally expensive and limits number of architectures that can be explored    Cell based Search Space:  the NASNet search space (Zoph et al. 2018) defines the architecture of a conv net as the same cell getting repeated multiple times and each cell contains several operations predicted by the NAS algorithm. a small directed acyclic graph representing a feature transformation NASNet search space: Learns 2 types of cells:  Normal Cell: Input and output feature maos have same dimension Reduction Cell: Output feature map has width the height reduced by half     Zoph et al., Learning Transferable Architectures for Scalable Image Recognition, CVPR 2018  pros  A well-designed cell module enables transferability between datasets. easy to scale down or up the model size by adjusting the number of cell repeats.  cons  arrangement of operation is restricted Suppose CNN can be obtained by stacking the same Cell, and RNN can be obtained by recursive connection of the same Cell  Comparision of different methods  Comparison with state-of-the-art architectures on ImageNet (mobile setting)  Table source: Chen +,Progressive Differentiable Architecture Search:Bridging the Depth Gap between Search and Evaluation, ICCV 2019\n    Reinforcement Learning Evolution Algorithm Differentiable Search     Computation cost High High Low   Search space Large Large Restricted    Differentiable Architecture Search: Gradient-based method Liu et al., DARTS: Differentiable Architecture Search, ICLR 2019\nOverview of DARTS  (a) Operations on the edges are initially unknown (b) Continuous relaxation of the search space by placing a mixture of candidate opeartions on each edge. (c) Joint optimization of the mixing probabilities and the network weights by solving a bilevel optimization problem. (d) Inducing the final architecture from the learned mixing probabilities  Continuous relaxation and optimization Relaxation  The operation mixing weights for a pair of nodes $(i, j)$ are parameterized by a vector in $\\alpha^{(i, j)}$ of dimension $ |\\mathcal O|$ The method relaxes categorical chioce of a particular operation as a softmax over all operations  $$ \\overline{o} ^{(i,j)}(x) = \\sum_{o\\in \\mathcal O}\\frac{exp(\\alpha_{o}^{ij})} { \\sum_{o^{'} \\in \\mathcal O} exp(\\alpha_{o^{'} } ^{(i,j)} ) }o(x)$$\n This task of architecture search then reduces to learning a set of continuous variables $\\alpha = \\lbrace\\alpha^{(i, j)} \\rbrace$ At the end of search, obatain discrete architecture by replacing each mixed operation $ { \\overline{o} }^{(i,j)} $ with the most likely operation, i.e. $o^{(i,j)} = \\underset{o \\in \\mathcal O}{argmax} \\ \\alpha_{o}^{(i, j)} $  Optimization   After relaxation, our goal is to jointly learn architecture $\\alpha$ and the weights $w$ within all the mixed operations (e.g. weight of the convolution filters)\n  The goal for archiutecture is to find $\\alpha^{*}$ that minimizes the validation loss $\\mathcal L_{train} (w^{*}, \\alpha^{*})$,\n  where the weights $ w^{*} $ assiciated with the architecture are obtained by minimizing the train loss $w^{*} = \\underset{w}{argmin} \\mathcal L_{train} (w, \\alpha^{*}) $\n  This implies a bilevel optimization problem with $\\alpha$ as the upper-level variable and $w$ as the lower-level variable\n  $$ \\underset{\\alpha}{min} {\\mathcal L}_{val}(w^{*}(\\alpha), \\alpha) $$\n$$ s.t. \\ w^{*} (\\alpha) = \\underset{w}{argmin} \\mathcal L_{train} (w, \\alpha) $$\nApproximate Architecture Gradient  evalute the gradient exactly can be prohibitive due to the inner optimizartion is expensive. an approximation scheme as follows:  $$ \\nabla_{\\alpha} {\\mathcal L_{val}(w^{*}(\\alpha), \\alpha)}\\approx \\nabla_{\\alpha}{\\mathcal L_{val}(w - \\xi \\nabla_{w}{\\mathcal L_{train} (w,\\alpha), \\alpha} )}$$\n  the idea is to approximate $w^{*}(\\alpha)$ by adapting $w$ using only a single training step using learning rate $\\xi$ without solving the inner optimization $w^{*} (\\alpha) = \\underset{w}{argmin} \\mathcal L_{train} (w, \\alpha) $ completely by training until convergence.\n  The procedure is outlined as Algotithm 1:\n    Apply chain rule to the approximate architecture gradient can get: $$\\color{red}{\\nabla_{\\alpha}}{\\mathcal L_{val}(w - \\xi \\nabla_{w}{\\mathcal L_{train} (w,\\color{red}{\\alpha}), \\color{red}{\\alpha}} )} = \\color{blue}{\\nabla_{\\alpha}} {\\mathcal L_{val}(w^{'},\\color{blue}{\\alpha})} - \\xi \\nabla_{\\alpha, w}^{2} {\\mathcal L_{train}}(w,\\alpha)\\cdot \\color{blue}{\\nabla_{w^{'}}} {\\mathcal L_{val}}(\\color{blue}{w^{'}},\\alpha)$$\n  where $w^{'} = w - \\xi \\nabla_{w}\\mathcal L_{train} (w,\\alpha)$ denoted the weights for a one-step forward model.\n   Induction: $$ \\nabla_{\\alpha} f(g_{1}(\\alpha), g_{2}(\\alpha))=\\color{blue}{D_{1}f(g_{1}(\\alpha), g_{2}(\\alpha))} \\cdot \\color {red}{\\nabla_{\\alpha} g_{1}(\\alpha)} +\\color{blue}{D_{2}f(g{1}(\\alpha), g_{2}(\\alpha))} \\cdot \\color {red}{\\nabla_{\\alpha} g_{2}(\\alpha)} $$\n $f(\\cdot,\\cdot) = {\\mathcal L{val}} (\\cdot, \\cdot) $ $g_{1}(\\alpha) = w^{'} = w - \\xi \\nabla{w}\\mathcal L{train} (w,\\alpha)$ $g_{2}(\\alpha) = \\alpha$  Differentiate:\n $\\nabla_{\\alpha} g_{1}(\\alpha) = - \\xi \\nabla{\\alpha, w}^{2} \\mathcal L{train} (w,\\alpha)$ $\\nabla_{\\alpha} g_{2}(\\alpha) = 1$ $D{1}f(g_{1}(\\alpha), g_{2}(\\alpha)) = \\color{blue}{\\nabla{w^{'}}} {\\mathcal L{val}}(\\color{blue}{w^{'}},\\alpha)$ $D{2}f(g_{1}(\\alpha), g_{2}(\\alpha)) = \\color{blue}{\\nabla{\\alpha}} {\\mathcal L{val}(w^{'},\\color{blue}{\\alpha})}$     the expression above contains an expensive matrix-vector product in its second term. Reduce it using finite difference approximation. Let $\\epsilon$ be a small scalar $\\epsilon = \\frac {0.01} { {\\lVert \\color{blue}{\\nabla{\\alpha}} {\\mathcal L{val}(w^{'},\\color{blue}{\\alpha})} \\rVert}_{ 2 } }$\n  and $w^{\\pm} = w \\pm \\epsilon \\color{blue}{\\nabla_{w^{'}}} {\\mathcal L_{val}}(\\color{blue}{w^{'}},\\alpha) $\n  Then\n$$\\nabla_{\\alpha, w}^{2} {\\mathcal L_{train}}(w,\\alpha)\\cdot \\color{blue}{\\nabla_{w^{'}}} {\\mathcal L_{val}}(\\color{blue}{w^{'}},\\alpha) \\approx \\frac {\\nabla_{\\alpha} {\\mathcal L_{train}} (w^{+}, \\alpha) - \\nabla_{\\alpha} {\\mathcal L_{train}} (w^{-}, \\alpha) } {2 \\epsilon} $$\n  the Complexity reduced from $O(|\\alpha | |w|)$ to $O(|\\alpha | + |w|)$\n   Induction: We know the Taylor series : $$ f(x) = f(x_0) + \\frac {f^{'}(x_0)}{ 1!} (x-x_0)+ \\frac {f^{''}(x_0)}{ 2!} (x-x_0)^2 + \\cdots$$\nLet $ x = x_0 + hA$ and $ x = x_0 - hA $, we can induct the following expressions:\n $f(x_{0} + hA) = f(x_0) + \\frac {f^{'} (x_0) }{ 1!}hA + \\cdots $ $ f(x_{0} - hA) = f(x_0) -\\frac {f^{'} (x_0) }{ 1!}hA + \\cdots $ $f^{'} (x_{0} )\\cdot A \\approx \\frac {f(x_{0} + hA )-f(x_{0} - hA )} { 2h } $  Then replace items by :\n $h = \\epsilon$ $A = \\color{blue}{\\nabla_{w^{'}}} {\\mathcal L_{val}}(\\color{blue}{w^{'}},\\alpha) $ $x_{0} = w$ $f(\\cdot, \\cdot) = \\nabla_{\\alpha} {\\mathcal L_{train}(\\cdot, \\cdot)}$   experiment and result experiment setting  following operations are included in $\\mathcal O$  3 × 3 dilated separable convolution  5 × 5 dilated separable convolution 3 × 3 depth-wise separable convolution 5 × 5 depth-wise separable convolution 3 × 3 max pooling 3 × 3 average pooling, no connection (zero) and a skip connection (identity)    result   CNN   RNN   The performanc by ENAS(RL method) looks similar to the NAS, I will explore it next time.\n  Pham et al., Efficient Neural Architecture Search via Parameter Sharing , ICML 2018\n  Conclusion  DARTS is able to greatly reduce the cost of GPU hours. Their experiments for searching for CNN cells have 7 Nodes and only took 1.5 days with a single GPU. However, it suffers from the high GPU memory consumption issue due to its continuous representation of network architecture.  Candidate NAS for my research  Random Search (as baseline) DARTS (Gradient) ENAS (RL+ parameter sharing)  Possible future direction  Search efficiency Moving towards less constrained Search Space Designing efficient architectures: automated scaling, pruning and quantization (model compression techniques metioned at last bw meeting)  Material To learn about NAS: [1] https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html#evolutionary-algorithms [2] https://hangzhang.org/ECCV2020/\nEngineering part  The implementation of DARTS is available at https://github.com/quark0/darts Apply it on my current research domain to see the performance?  "
    }
,
    {
        "ref": "https://bing-bai.github.io/blog/posts/hyper-parameter-optimization/",
        "title": "Hyper Parameter Optimization",
        "section": "posts",
        "tags": ["markdown","machine learning"],
        "date" : "2021.06.19",
        "body": "Hyper-parameter Optimization HP categories to be optimized  Table of principal HPs grouped by role and domain     hp-parameter example     continuous learning rate   Discrete poolSize, kernal size, sliding, hcHidden, data batch, numEpoch   categorical Choice of optimizer, Activation function   binary whether to early stop    Existing Methods summary Table. 1: comparison of common HPO algorithm (n is the number of hyper-parameter values and k is the number of hyper-parameters)\n   HPO methods Strengths Limitations Time Complexity     Grid Search Simple Time consuming; Only efficient with categorical HPs O(n^k)   Random Search More efficient than GS Not consider previous results; Not efficient with conditional HPs. O(n)   Gradient-based models Fast convergence speed for continuous HPs Only support continuous HPs; May only detect local optimums O(n^k)   BO-GP Fast convergence speed for continuous HPs Poor capacity for parallelization; Not efficient with conditional HPs. O(n^3)   SMAC Efficient with all types of HPs Poor capacity for parallelization O(nlog(n))   BO-TPE Efficient with all types of HPs; Keep conditional dependencies. Poor capacity for parallelization O(nlog(n))   Hyperband Enable parallelization Not efficient with conditional HPs; Require subsets with small budgets to be representative O(nlog(n))   BOHB Efficient with all types of HPs Require subsets with small budgets to be representative. O(nlog(n))   GA Efficient with all types of HPs; Not require good initialization Poor capacity for parallelization. O(n^2)   PSO Efficient with all types of HPs; Enable parallelization Require proper initialization O (nlogn)    Yang+, On hyperparameter optimization of machine learning algorithms: Theory and practice, Neurocomputing 2020\nGrid search Grid Search performs an exhaustive search through a manually specified subset of the hyperparameter space defined in the searchspace file. https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\nRandom Search Random Search might be surprisingly effective despite its simplicity. We suggest using Random Search as a baseline when no knowledge about the prior distribution of hyper-parameters is available. Bergstra+, Random Search for Hyper-Parameter Optimization, Journal of Machine Learning Research Vol.13, pp. 281-305, 2012\nGradient-based optimization First randomly selecting a data point, then moves towards the opposite direction of the largest gradient to locate the next data point. Therefore, a local optimum can be reached after convergence. The local optimum is also the global optimum for convex functions.\nBayesian optimization   summary: BO determines the future evaluation points based on the previously obtained results. To determine the next hyper-parameter configuration, BO uses two key components: a surrogate model and an acquisition function.\n  procedure\n   Build a probabilistic surrogate model of the objective function.    Detect the optimal hyper-parameter values on the surrogate model.    Apply these hyper-parameter values to the real objective function to evaluate them.    Update the surrogate model with new results.    Repeat steps 2–4 until the maximum number of iterations is reached.      BO-GP https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf This is a sequential model-based optimization approach with Gaussian Process as the surrogate.\n  SMAC https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf SMAC is based on Sequential Model-Based Optimization (SMBO). It adapts the most prominent previously used model class (Gaussian stochastic process models) and introduces the model class of random forests to SMBO in order to handle categorical parameters.\n  BO-TPE (Tree-structured Parzen Estimator) https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf The Tree-structured Parzen Estimator (TPE) is a sequential model-based optimization (SMBO) approach. SMBO methods sequentially construct models to approximate the performance of hyperparameters based on historical measurements, and then subsequently choose new hyperparameters to test based on this model. The TPE approach models P(x|y) and P(y) where x represents hyperparameters and y the associated evaluation matric. P(x|y) is modeled by transforming the generative process of hyperparameters, replacing the distributions of the configuration prior with non-parametric densities.\n  Hyperband https://arxiv.org/pdf/1603.06560.pdf Hyperband tries to use limited resources to explore as many configurations as possible and returns the most promising ones as a final result. The basic idea is to generate many configurations and run them for a small number of trials. The half least-promising configurations are thrown out, the remaining are further trained along with a selection of new configurations. The size of these populations is sensitive to resource constraints (e.g. allocated search time).\n  BOHB(Bayesian Optimization Hyper Band) https://arxiv.org/pdf/1807.01774.pdf BO is an abbreviation for “Bayesian Optimization” and HB is an abbreviation for “Hyperband”. BOHB relies on HB to determine how many configurations to evaluate with which budget, but it replaces the random selection of configurations at the beginning of each HB iteration by a model-based search (Bayesian Optimization). Once the desired number of configurations for the iteration is reached, the standard successive halving procedure is carried out using these configurations. We keep track of the performance of all function evaluations g(x, b) of configurations x on all budgets b to use as a basis for our models in later iterations. HB part Follow HyperBand\u0026rsquo;s way of choosing the budgets and continue to use SuccessiveHalving. It is initialized with a set of configurations, a minimum and maximum budget, and a scaling parameter η. In the first stage all configurations are evaluated on the smallest budget (line3). The losses are then sorted and only the best 1/η configurations are kept in the set C (line 4). For the following stage, the budget is increased by a factor of η (line 5). This is repeated until the maximum budget for a single configuration is reached (line 2). BO part Tree Parzen Estimator(TPE): uses a KDE (kernel density estimator) to model the densities.   Multidimensional KDE is used to guide the selection of configurations for the next iteration. The sampling procedure (using Multidimensional KDE to guide selection) is summarized by the pseudocode below. To fit useful KDEs, we require a minimum number of data points Nmin; this is set to d + 1 for our experiments, where d is the number of hyperparameters. To build a model as early as possible, we do not wait until Nb = |Db|, where the number of observations for budget b is large enough to satisfy q · Nb ≥ Nmin. Instead, after initializing with Nmin + 2 random configurations, we choose the best and worst configurations respectively to model the two densities. Workflow This image shows the workflow of BOHB. Here we set max_budget = 9, min_budget = 1, eta = 3, others as default. In this case, s_max = 2, so we will continuously run the {s=2, s=1, s=0, s=2, s=1, s=0, …} cycle. In each stage of SuccessiveHalving (the orange box), we will pick the top 1/eta configurations and run them again with more budget, repeating the SuccessiveHalving stage until the end of this iteration. At the same time, we collect the configurations, budgets and final metrics of each trial and use these to build a multidimensional KDE model with the key “budget”.\nopensourced code\nMetaheuristic algorithms  GA (genetic algorithm) PSO (Particle Swarm Optimization)  "
    }
,
    {
        "ref": "https://bing-bai.github.io/blog/posts/modelcompression/",
        "title": "Model Compression",
        "section": "posts",
        "tags": ["markdown","machine learning"],
        "date" : "2021.06.19",
        "body": "Model Compression Introduction Choudhary+, [A comprehensive survey on model compression and acceleration](https://link.springer.com/content/pdf/10.1007/s10462-020-09816-7.pdf), Artifcial Intelligence Review 2020 Table 2: summary of different methods for network compression.\n   Method strengths limitations     Knowledge Distill Can downsize a network regardless of the structural difference between the teacher and the student network can only be applied to classification tasks with softmax loss function   Low-Rank Factorization standard pipepline performed layer by layer, cannot perform global parameter compression   Data quantization significantly reduced memory usage and float-point operations Quantized weights make neural networks harder to converge   Pruning Can improve the inference time and model size vs accuracy tradeoff for a given architecture Generally, does not help as much as switching to a better architecture    Cheng+, Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges, Vol.35, pp.126-136, 2018\nLow-Rank Factorization In low-rank factorization, a weight matrix A with m × n dimension and having rank r is replaced by smaller dimension matrices. In feed-forward NN and CNN, singular value decomposition (SVD) is a common and popular factorization scheme for reducing the number of parameters. SVD factorize the original weight matrix into three smaller matrices, replacing the original weight matrix. For any matrix A ∈ ℝm×n , there exists a factorization, A = U S V^(T) . Where, U ∈ ℝ^(m×r) , S ∈ ℝ^(r×r) , and V^T ∈ ℝ^(r×n) . S is a diagonal matrix with the singular values on the diagonal, U and V are orthogonal matrices. Each entry in the S is larger than the next diagonal entry. When reducing the model size is necessary, low- rank factorization techniques help by factorizing a large matrix into smaller matrices. Yu+, On Compressing Deep Models by Low Rank and Sparse Decomposition\nKnowledge Distillation Knowledge Distilled: The basic idea of KD is to distill knowledge from a large teacher model into a small one by learning the class distributions output by the teacher via softened softmax.\nMishra+, Apprentice: using knowledge distillation techniques to improve low-precision network accuracy, ICLR2018\ndata quantization In quantization, we represent weights by reducing the number of bits required per weight to store each weight. This idea can also be further extended to represent gradient and activation in the quantized form. The weights can be quantized to 16-bit, 8-bit, 4-bit or even with 1-bit (which is a particular case of quantization, in which weights are represented with binary values only, known as weight binarization).\nour model is implemented by scala, the parameter is indouble: 64 bit, float: 32 bit, integer: 32 bit.\n  QAT Quantizer Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, CVPR2018\n Weights are quantized before they are convolved with the input. If batch normalization is used for the layer, the batch normalization parameters are “folded into” the weights before quantization. Activations are quantized at points where they would be during inference, e.g. after the activation function is applied to a convolutional or fully connected layer’s output, or after a bypass connection adds or concatenates the outputs of several layers together such as in ResNets.    BNN Quantizer Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\n  pruning pruning: remove parameters which have least effect on the accuracy of the network, which can reduce model complexity and mitigate the over-ﬁtting issue.\n  Weight pruning: In unimportant weight connection pruning, we prunes (zeros out) the weight connections if they are below some predefned threshold (Han et al. 2015) or if they are redundant. Han+, Learning both Weights and Connections for Efficient Neural Networks, NIPS 2015\n  Neuron pruning: Instead of removing the weights one by one, which is a timeconsuming process, we can also remove the individual neurons if they are redundant (Srinivas and Babu 2015). In that case, all the incoming and outgoing connection of the neuron will also be removed. There are many other ways of removing individual weight connections or neurons. Srinivas+, Data-free parameter pruning for Deep Neural Networks, BMVC 2015\n  Filter pruning: In flter pruning, flters are ranked according to their importance, and the least important (least ranking) flters are removed from the network. The importance of the flters can be calculated by L1/L2 norm (Li et al. 2017) or some other methods like their infuence on the error. Li+, Pruning Filters for Efficient ConvNets, ICLR 2017\n  Layer pruning: Similarly, from a very deep network, some of the layers can also be pruned (Chen and Zhao 2018). Chen+, Shallowing Deep Networks: Layer-Wise Pruning Based on Feature Representations, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 41, Iss. 12, pp. 3048 - 3056, 2019\n  criteria The stsndard criteria to measure the quality of model compression and acceleration are the compression rate and the speedup rate. table source: Choudhary+, A comprehensive survey on model compression and acceleration, Artifcial Intelligence Review 2020\n"
    }
]
