<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bing BAI</title>
    <link>https://bing-bai.github.io/blog/</link>
    <description>Recent content on Bing BAI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Jul 2021 00:44:32 +0900</lastBuildDate><atom:link href="https://bing-bai.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Differentiable Architecture Search</title>
      <link>https://bing-bai.github.io/blog/posts/differentiable-architecture-search/</link>
      <pubDate>Sun, 11 Jul 2021 00:44:32 +0900</pubDate>
      
      <guid>https://bing-bai.github.io/blog/posts/differentiable-architecture-search/</guid>
      <description>code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}  MathJax.Hub.Config({ tex2jax: { inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]], skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;] // removed &#39;code&#39; entry } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i = 0; i  NAS: General Problem Setup Overview   source: Elksen et al., Neural Architecture Search: A survey, 2018
  Search Space: Defines which architecture can be represented in principle,
  Search Strategy: Detail on how to explore search space</description>
    </item>
    
    <item>
      <title>Hyper Parameter Optimization</title>
      <link>https://bing-bai.github.io/blog/posts/hyper-parameter-optimization/</link>
      <pubDate>Sat, 19 Jun 2021 21:24:38 +0900</pubDate>
      
      <guid>https://bing-bai.github.io/blog/posts/hyper-parameter-optimization/</guid>
      <description>Hyper-parameter Optimization HP categories to be optimized  Table of principal HPs grouped by role and domain     hp-parameter example     continuous learning rate   Discrete poolSize, kernal size, sliding, hcHidden, data batch, numEpoch   categorical Choice of optimizer, Activation function   binary whether to early stop    Existing Methods summary Table. 1: comparison of common HPO algorithm (n is the number of hyper-parameter values and k is the number of hyper-parameters)</description>
    </item>
    
    <item>
      <title>Model Compression</title>
      <link>https://bing-bai.github.io/blog/posts/modelcompression/</link>
      <pubDate>Sat, 19 Jun 2021 18:39:43 +0900</pubDate>
      
      <guid>https://bing-bai.github.io/blog/posts/modelcompression/</guid>
      <description>Model Compression Introduction Choudhary+, [A comprehensive survey on model compression and acceleration](https://link.springer.com/content/pdf/10.1007/s10462-020-09816-7.pdf), Artifcial Intelligence Review 2020 Table 2: summary of different methods for network compression.
   Method strengths limitations     Knowledge Distill Can downsize a network regardless of the structural difference between the teacher and the student network can only be applied to classification tasks with softmax loss function   Low-Rank Factorization standard pipepline performed layer by layer, cannot perform global parameter compression   Data quantization significantly reduced memory usage and float-point operations Quantized weights make neural networks harder to converge   Pruning Can improve the inference time and model size vs accuracy tradeoff for a given architecture Generally, does not help as much as switching to a better architecture    Cheng+, Model Compression and Acceleration for Deep Neural Networks: The Principles, Progress, and Challenges, Vol.</description>
    </item>
    
    <item>
      <title>Profile</title>
      <link>https://bing-bai.github.io/blog/posts/profile/</link>
      <pubDate>Sat, 19 Jun 2021 18:34:05 +0900</pubDate>
      
      <guid>https://bing-bai.github.io/blog/posts/profile/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
